#!/usr/bin/env python3
"""
Classification ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
Confusion Matrix ë° Classification Report ìƒì„±
"""

import argparse
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT = Path(__file__).parent
sys.path.append(str(ROOT))

from core.config import ConfigLoader
from core.evaluator import ClassificationEvaluator
from ultralytics import YOLO


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='Classification ëª¨ë¸ í‰ê°€',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í‰ê°€ (ê²€ì¦ ë°ì´í„°ì…‹)
  python evaluate.py --config configs/models/cls/csn.yaml --weights classify/csn_cls_11s_25121910h/weights/best.pt
  
  # íŠ¹ì • ê²€ì¦ ë””ë ‰í† ë¦¬
  python evaluate.py --config configs/models/cls/csn.yaml --weights best.pt --val-dir /path/to/validation
  
  # ê²°ê³¼ ì €ì¥ ì•ˆ í•¨ (ì½˜ì†”ë§Œ)
  python evaluate.py --config configs/models/cls/csn.yaml --weights best.pt --no-save
        """
    )
    
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (Classification ì „ìš©)'
    )
    
    parser.add_argument(
        '--weights',
        type=str,
        required=True,
        help='í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²½ë¡œ (.pt íŒŒì¼)'
    )
    
    parser.add_argument(
        '--val-dir',
        type=str,
        default=None,
        help='ê²€ì¦ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì—ì„œ ìë™ ì„¤ì •)'
    )
    
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='ê²°ê³¼ ì €ì¥ ì•ˆ í•¨ (ì½˜ì†”ë§Œ ì¶œë ¥)'
    )
    
    return parser.parse_args()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()
    
    print("=" * 60)
    print("Classification ëª¨ë¸ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        # 1. ì„¤ì • ë¡œë“œ
        print(f"\n[1/4] ì„¤ì • ë¡œë“œ ì¤‘...")
        config_loader = ConfigLoader(args.config, task='classify')
        config = config_loader.load()
        
        if config['task'] != 'classify':
            print(f"\nâŒ Classification ëª¨ë¸ë§Œ í‰ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤ (í˜„ì¬: {config['task']})")
            sys.exit(1)
        
        print(f"  âœ“ Product: {config['product']}")
        print(f"  âœ“ Model: {config['model']}")
        
        # 2. ëª¨ë¸ ë¡œë“œ
        print(f"\n[2/4] ëª¨ë¸ ë¡œë“œ ì¤‘...")
        weights_path = Path(args.weights)
        if not weights_path.exists():
            raise FileNotFoundError(f"ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.weights}")
        
        model = YOLO(str(weights_path))
        print(f"  âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # 3. Evaluator ì´ˆê¸°í™”
        print(f"\n[3/4] Evaluator ì´ˆê¸°í™” ì¤‘...")
        evaluator = ClassificationEvaluator(config, model)
        
        # 4. í‰ê°€ ì‹¤í–‰
        print(f"\n[4/4] í‰ê°€ ì‹¤í–‰...")
        print("-" * 60)
        
        metrics = evaluator.evaluate(
            val_dir=args.val_dir,
            save_results=not args.no_save
        )
        
        print("-" * 60)
        
        # 5. ì™„ë£Œ
        print("\n" + "=" * 60)
        print("âœ… í‰ê°€ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š Overall Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"ğŸ“ˆ í´ë˜ìŠ¤ ìˆ˜: {len(metrics['class_names'])}")
        print(f"ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜: {len(metrics['y_true'])}")
        
        if not args.no_save:
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨:")
            print(f"  - Confusion Matrix (PNG)")
            print(f"  - Classification Report (TXT)")
        
        print("=" * 60)
        
    except FileNotFoundError as e:
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
